{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9f0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d526e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットをダウンロード\n",
    "(X, y), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804c1a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 平滑化\n",
    "X_flat = X.reshape(-1, 784)\n",
    "X_test_flat = X_test.reshape(-1, 784)\n",
    "print(X_flat.shape)\n",
    "print(X_test_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8213ec1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\masahiro_umeda\\AppData\\Local\\Temp\\ipykernel_19340\\4222454172.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_flat = X_flat.astype(np.float)\n",
      "C:\\Users\\masahiro_umeda\\AppData\\Local\\Temp\\ipykernel_19340\\4222454172.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_test_flat = X_test_flat.astype(np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 型変換、正規化\n",
    "X_flat = X_flat.astype(np.float)\n",
    "X_test_flat = X_test_flat.astype(np.float)\n",
    "X_flat /= 255\n",
    "X_test_flat /= 255\n",
    "print(X_flat.max())\n",
    "print(X_flat.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64291f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\masahiro_umeda\\anaconda3\\envs\\lesson\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 正しいラベル値のワンホット エンコーディング\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y.shape)\n",
    "print(y_one_hot.shape)\n",
    "print(y_one_hot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd9a903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n",
      "(48000, 10)\n",
      "(12000, 10)\n"
     ]
    }
   ],
   "source": [
    "# トレーニング データと検証データに分割\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_flat, y_one_hot, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12342df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98abf368",
   "metadata": {},
   "source": [
    "# 問題1　全結合層のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea622b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, activation):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.optimizer = optimizer\n",
    "        self.initializer = initializer\n",
    "        self.activation = activation\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(self.n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(self.n_nodes1)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.A = np.dot(X, self.W) + self.B\n",
    "        self.Z = self.activation.forward(sel.A)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dA = self.dZ * (1 - sigmoid(self.Z))*sigmoid(self.Z)\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b2c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8b1b0bd",
   "metadata": {},
   "source": [
    "# 問題2　初期化方法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e419d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        weight = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return weight\n",
    "    \n",
    "    def B(self, n_nodes1):\n",
    "        bias = np.zeros(n_nodes1)\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041f6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29b1f322",
   "metadata": {},
   "source": [
    "# 問題3　最適化手法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47cf9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr*layer.dW\n",
    "        layer.B -= self.lr*layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cca0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acf8a407",
   "metadata": {},
   "source": [
    "# 問題4　活性化関数のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8c3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        self.Z = np.tanh(self.A)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        return dZ*(1-self.Z**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89ffc028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        self.Z = 1/(1+np.exp(-self.A))\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ*(1-self.Z)*self.Z\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7be52dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        return np.exp(A-np.max(A))/np.sum(np.exp(A-np.max(A)), axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e769d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be13d879",
   "metadata": {},
   "source": [
    "# 問題5　ReLUクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d84fc78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.maximum(self.A,0)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return np.where(self.A>0, dZ, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55afda5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64775ea8",
   "metadata": {},
   "source": [
    "# 問題6　重みの初期値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a030c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        return np.random.randn(n_nodes1, n_nodes2)/np.sqrt(n_nodes1)\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        return np.zeros(n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f3d4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        return np.random.randn(n_nodes1, n_nodes2)*np.sqrt(2/n_nodes1)\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        return np.zeros(n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15c3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "487f1b21",
   "metadata": {},
   "source": [
    "# 問題7　最適化手法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5b5cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad():\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.hW = 0\n",
    "        self.hB = 0\n",
    "        \n",
    "    def update(self, layer):\n",
    "        self.hW += layer.dW*layer.dW\n",
    "        self.hB = layer.dB*layer.dB\n",
    "        \n",
    "        layer.W -= self.lr*layer.dW/(np.sqrt(self.hW) + 1e-7)\n",
    "        layer.B -= self.lr*layer.dB/(np.sqrt(self.hB) + 1e-7)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806dc338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "266b3d76",
   "metadata": {},
   "source": [
    "# 問題8　クラスの完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0de6bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetworkClassifier():\n",
    "    def __init__(self, n_features, n_nodes1, n_nodes2, n_output, sigma, n_epoch, n_batch, lr, verbose=False):\n",
    "        # Parameters\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "        self.n_epoch = n_epoch\n",
    "        self.n_batch = n_batch\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.log_loss = np.zeros(self.n_epoch)\n",
    "        \n",
    "    def loss_function(self, y, yt):\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt*np.log(y+delta))\n",
    "                \n",
    "    def fit(self, X, y, X_val=False, y_val=False):\n",
    "        optimizer1 = AdaGrad(self.lr)\n",
    "        optimizer2 = AdaGrad(self.lr)\n",
    "        optimizer3 = AdaGrad(self.lr)\n",
    "        \n",
    "        initializer1 = XavierInitializer()\n",
    "        initializer2 = XavierInitializer()\n",
    "        initializer3 = SimpleInitializer(self.sigma)\n",
    "        \n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, initializer1, optimizer1, Tanh())\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, initializer2, optimizer2, Tanh())\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, initializer3, optimizer3, Softmax())\n",
    "        \n",
    "        for epoch in range(self.n_epoch):\n",
    "            # ミニバッチ処理\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
    "            \n",
    "            self.loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                \n",
    "                # フォワードプロパゲーション\n",
    "                self.Z1 = self.FC1.forward(mini_X_train)\n",
    "                self.Z2 = self.FC2.forward(self.Z1)\n",
    "                self.Z3 = self.FC3.forward(self.Z2)    \n",
    "\n",
    "                # バックプロパゲーション\n",
    "                self.dA3 = (self.Z3 - mini_y_train)/self.n_batch\n",
    "                self.dZ2 = self.FC3.backward(self.dA3)\n",
    "                self.dZ1 = self.FC2.backward(self.dZ2)\n",
    "                self.dZ0 = self.FC1.backward(self.dZ1)\n",
    "                \n",
    "                # 損失関数\n",
    "                self.loss += self.loss_function(self.Z3,mini_y_train)\n",
    "                \n",
    "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        pred_Z1 = self.FC1.forward(X)\n",
    "        pred_Z2 = self.FC2.forward(pred_Z1)\n",
    "        return np.argmax(self.FC3.forward(pred_Z2),axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05ee62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aea55f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチ処理クラス\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形の ndarray、形状 (n_samples、n_features)\n",
    "        訓練データ\n",
    "    y : 次の形の ndarray、形状 (n_samples, 1) \n",
    "        正しい値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyでの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=None):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1] \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a5e78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ScratchDeepNeuralNetworkClassifier(n_features=784, n_nodes1=400, n_nodes2=200, \n",
    "                                         n_output=10, sigma=0.01, n_epoch=5, n_batch=100, lr=0.01, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3753f567",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m      3\u001b[0m y_pred\n",
      "Cell \u001b[1;32mIn[17], line 28\u001b[0m, in \u001b[0;36mScratchDeepNeuralNetworkClassifier.fit\u001b[1;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[0;32m     25\u001b[0m initializer2 \u001b[38;5;241m=\u001b[39m XavierInitializer()\n\u001b[0;32m     26\u001b[0m initializer3 \u001b[38;5;241m=\u001b[39m SimpleInitializer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma)\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFC1 \u001b[38;5;241m=\u001b[39m \u001b[43mFC\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_nodes1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFC2 \u001b[38;5;241m=\u001b[39m FC(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes2, initializer2, optimizer2, Tanh())\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFC3 \u001b[38;5;241m=\u001b[39m FC(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_output, initializer3, optimizer3, Softmax())\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_valid)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec1a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
